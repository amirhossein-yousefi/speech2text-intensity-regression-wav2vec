# Speech Multitask End-to-End (ASR + Intensity Regression)

This project trains and serves **two** end-to-end speech models that jointly perform:
- **Speech-to-Text (ASR)** and
- **Voice intensity regression** (in dBFS normalized to [0,1]) from the same audio.


## Project Structure
```
speech_mtl_end2end/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.cfg
â”œâ”€â”€ Makefile
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ whisper_base.yaml
â”‚   â””â”€â”€ wav2vec2_base.yaml
â”œâ”€â”€ app/
â”‚   â””â”€â”€ gradio_app.py
â”œâ”€â”€ src/
â”‚   â””â”€â”€ speech_mtl/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ utils/
â”‚       â”‚   â”œâ”€â”€ audio.py
â”‚       â”‚   â””â”€â”€ metrics.py
â”‚       â”œâ”€â”€ data/
â”‚       â”‚   â”œâ”€â”€ datasets.py
â”‚       â”‚   â””â”€â”€ collators.py
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ multitask_whisper.py
â”‚       â”‚   â””â”€â”€ multitask_wav2vec2.py
â”‚       â”œâ”€â”€ training/
â”‚       â”‚   â”œâ”€â”€ train_whisper.py
â”‚       â”‚   â””â”€â”€ train_wav2vec2.py
â”‚       â”œâ”€â”€ eval/
â”‚       â”‚   â””â”€â”€ evaluate.py
â”‚       â””â”€â”€ inference/
â”‚           â””â”€â”€ predict.py
â””â”€â”€ tests/
    â””â”€â”€ test_audio_utils.py
â””â”€â”€ sagemaker/
    â”œâ”€â”€ README.md                     
    â”œâ”€â”€ deploy_custom.py              
    â”œâ”€â”€ package_model.py              
    â”œâ”€â”€ client/
    â”‚   â””â”€â”€ invoke.py                
    â””â”€â”€ code/
        â”œâ”€â”€ inference.py              
        â””â”€â”€ requirements.txt 
```

## Approaches
1. **Whisper + Regression head** (Seq2Seq): `src/speech_mtl/models/multitask_whisper.py`
2. **Wav2Vec2-CTC + Regression head**: `src/speech_mtl/models/multitask_wav2vec2.py`

Both are trained using Hugging Face `transformers` and `datasets`.

## Dataset
Default: [Librispeech ASR](https://huggingface.co/datasets/librispeech_asr) (`train.clean.100` / `validation.clean` / `test.clean`).  
We compute **intensity labels** from the audio as **RMS dBFS** (bounded to [-60, 0]) and also provide a normalized target in `[0,1]`:
```
norm_intensity = (dbfs + 60) / 60
```
Optionally, you can switch to **LUFS** (requires `pyloudnorm`).

## Quickstart

### 0) Environment
```bash
python -m venv .venv
source .venv/bin/activate     # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

(Optional) For GPU training, also install a CUDA build of PyTorch per https://pytorch.org/get-started/locally/

### 1) Train Whisper (ASR + intensity)
```bash
python -m src.speech_mtl.training.train_whisper \
  --model_name openai/whisper-small \
  --language en \
  --dataset librispeech_asr \
  --train_split train.clean.100 \
  --eval_split validation.clean \
  --text_column text \
  --num_train_epochs 1 \
  --output_dir outputs/whisper_small_mtl
```

### 2) Train Wav2Vec2-CTC (ASR + intensity)
```bash
python -m src.speech_mtl.training.train_wav2vec2 
   --model_name facebook/wav2vec2-base-960h 
   --dataset librispeech_asr 
   --train_split train.clean.100 
   --eval_split validation.clean 
   --text_column text 
   --max_train_samples 1000 
   --max_eval_samples 150 
   --num_train_epochs 1 
   --output_dir outputs\wav2vec2_small
```
The training script validates each epoch.
 you can download the finetuned weights fo one epoch from https://drive.google.com/file/d/1opkvEG4GLVya6rxWb87CMhs3BD-GByZt/view?usp=sharing
the training logs for 1 epoch is found in [training-logs](training-logs)
## ğŸ–¥ï¸ Training Hardware & Environment

- **Device:** Laptop (Windows, WDDM driver model)  
- **GPU:** NVIDIA GeForce **RTX 3080 Ti Laptop GPU** (16 GB VRAM)  
- **Driver:** **576.52**  
- **CUDA (driver):** **12.9**  
- **PyTorch:** **2.8.0+cu129**  
- **CUDA available:** âœ… 

### 3) Evaluate
```bash
python -m src.speech_mtl.eval.evaluate \
  --whisper_model_dir outputs/whisper_small_mtl \
  --wav2vec2_model_dir outputs/wav2vec2_base_mtl \
  --dataset librispeech_asr --split test.clean --text_column text
```

### 4) Inference (CLI)
```bash
python -m src.speech_mtl.inference.predict \
  --model whisper \
  --checkpoint outputs/whisper_small_mtl \
  --audio path/to/audio.wav
```

### 5) Gradio Demo
```bash
python app/gradio_app.py --model whisper --checkpoint outputs/whisper_small_mtl
# or
python app/gradio_app.py --model wav2vec2 --checkpoint outputs/wav2vec2_base_mtl
```

### 6) Deploy to Amazon SageMaker

This project includes a production-style deployment flow using **Amazon SageMaker** with Hugging Face DLCs. You can package your trained checkpoint, deploy it as a serverless or real-time endpoint, and invoke it with audio files.
find more info and about how to deploy the model in sagemaker  [here](sagemaker).

## Notes
- **Intensity loss weight** is controlled by `--lambda_intensity`. Set `0.0` to disable.
- For **Whisper**, we use a mean-pooled encoder representation for the regression head.
- For **Wav2Vec2-CTC**, we use attention-masked mean pooling for the regression head.
- You can swap `librispeech_asr` with `mozilla-foundation/common_voice_13_0` by passing `--dataset` and `--language` accordingly.

## License
MIT
